# TODO implement all the attentions

# https://zh.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html#sec-attention-scoring-functions
# 1. masked attention
# 2. scaled dot product attention
# 3. additive attention

# https://zh.d2l.ai/chapter_attention-mechanisms/multihead-attention.html
# 4. multi head scaled dot product attention

# https://zh.d2l.ai/chapter_attention-mechanisms/transformer.html
# 5. transformer, compared with makemore's implementation

import torch
