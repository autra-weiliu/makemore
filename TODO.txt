### TODOs
[x] implement bigram test   
[ ] multigram mlp test   
[ ] replace `loss.backward()` with manually gradient calculation   
